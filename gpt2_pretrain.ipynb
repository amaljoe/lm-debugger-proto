{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-21T10:41:21.136069Z",
     "start_time": "2025-02-21T10:41:18.642468Z"
    }
   },
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, AdamW\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd*4)\n",
    "        self.act = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        return h2\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2Block, self).__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([GPT2Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.transformer['wte'](x)\n",
    "        pos = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        x = x + self.transformer['wpe'](pos)\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        x = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.reshape(-1))\n",
    "        return x, loss\n",
    "    \n",
    "    \n",
    "config = GPTConfig()\n",
    "our_model = GPT(config)\n",
    "hf_model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ],
   "outputs": [],
   "execution_count": 398
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T10:41:23.339225Z",
     "start_time": "2025-02-21T10:41:21.235660Z"
    }
   },
   "cell_type": "code",
   "source": "model = GPT.from_pretrained('gpt2')",
   "id": "6b34357af0a3c708",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    }
   ],
   "execution_count": 399
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T05:09:56.315898Z",
     "start_time": "2025-02-21T05:09:56.169280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows = []\n",
    "\n",
    "for k in our_model.state_dict().keys():\n",
    "    if k in hf_model.state_dict().keys():\n",
    "        rows.append(\n",
    "            {\n",
    "                'key': k,\n",
    "                'ours': our_model.state_dict()[k].shape,\n",
    "                'huggingface': hf_model.state_dict()[k].shape\n",
    "            }\n",
    "        )\n",
    "        \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ],
   "id": "d8977b694726d446",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                    key          ours   huggingface\n",
       "0                transformer.wte.weight  (50257, 768)  (50257, 768)\n",
       "1                transformer.wpe.weight   (1024, 768)   (1024, 768)\n",
       "2           transformer.h.0.ln_1.weight        (768,)        (768,)\n",
       "3             transformer.h.0.ln_1.bias        (768,)        (768,)\n",
       "4    transformer.h.0.attn.c_attn.weight   (2304, 768)   (768, 2304)\n",
       "..                                  ...           ...           ...\n",
       "144  transformer.h.11.mlp.c_proj.weight   (768, 3072)   (3072, 768)\n",
       "145    transformer.h.11.mlp.c_proj.bias        (768,)        (768,)\n",
       "146             transformer.ln_f.weight        (768,)        (768,)\n",
       "147               transformer.ln_f.bias        (768,)        (768,)\n",
       "148                      lm_head.weight  (50257, 768)  (50257, 768)\n",
       "\n",
       "[149 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>ours</th>\n",
       "      <th>huggingface</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformer.wte.weight</td>\n",
       "      <td>(50257, 768)</td>\n",
       "      <td>(50257, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transformer.wpe.weight</td>\n",
       "      <td>(1024, 768)</td>\n",
       "      <td>(1024, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformer.h.0.ln_1.weight</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformer.h.0.ln_1.bias</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformer.h.0.attn.c_attn.weight</td>\n",
       "      <td>(2304, 768)</td>\n",
       "      <td>(768, 2304)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>transformer.h.11.mlp.c_proj.weight</td>\n",
       "      <td>(768, 3072)</td>\n",
       "      <td>(3072, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>transformer.h.11.mlp.c_proj.bias</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>transformer.ln_f.weight</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>transformer.ln_f.bias</td>\n",
       "      <td>(768,)</td>\n",
       "      <td>(768,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>lm_head.weight</td>\n",
       "      <td>(50257, 768)</td>\n",
       "      <td>(50257, 768)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 379
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T10:41:53.964340Z",
     "start_time": "2025-02-21T10:41:53.536150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2_local')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "samples = 5\n",
    "tokens = tokenizer.encode(\"My wife is working as a\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(samples, 1) # (5, 8)\n",
    "while tokens.shape[1] < 10:\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(tokens)\n",
    "        logits = logits[:, -1, :] # (5, 50257)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk = torch.topk(logits, 5, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 20, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        tokens = torch.cat([tokens, xcol], dim=-1)\n",
    "    \n",
    "for i in range(tokens.shape[0]):    \n",
    "    print(\"> \" + tokenizer.decode(tokens[i].tolist()))"
   ],
   "id": "f23a31afea6df049",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> My wife is working as a consultant to make her\n",
      "> My wife is working as a cook at a restaurant\n",
      "> My wife is working as a nurse, and I\n",
      "> My wife is working as a carpenter. I\n",
      "> My wife is working as a journalist at the BBC\n"
     ]
    }
   ],
   "execution_count": 402
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T05:22:36.791106Z",
     "start_time": "2025-02-21T05:22:29.189184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "B, T = 5, 8\n",
    "data = open('women_in_tech.txt').read()\n",
    "data = tokenizer.encode(data)\n",
    "batches = [data[i:i + T + 1] for i in range(0, len(data), T) if len(data[i:i + T + 1]) == T + 1]\n",
    "batches = torch.tensor(batches, dtype=torch.long)\n",
    "if len(batches) % B != 0:\n",
    "    batches = batches[:-(len(batches) % B)]\n",
    "batches = batches.reshape(-1, B, T + 1) # (N, B, T+1)\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    cum_loss = 0\n",
    "    for batch in batches:\n",
    "        x = batch[:, :-1] # (B, T)\n",
    "        y = batch[:, 1:] # (B, T)\n",
    "        opt = AdamW(model.parameters(), lr=1e-4)\n",
    "        opt.zero_grad()\n",
    "        logits, loss = model(x, y)\n",
    "        cum_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    cum_loss /= len(batches)\n",
    "    print(f'epoch {epoch} loss: {cum_loss:.2f}')"
   ],
   "id": "b8d76628c66853cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 3.73\n",
      "epoch 1 loss: 2.03\n",
      "epoch 2 loss: 1.16\n"
     ]
    }
   ],
   "execution_count": 386
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T09:31:47.212122Z",
     "start_time": "2025-02-21T09:31:45.747701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.save_pretrained(\"gpt2_local\")"
   ],
   "id": "7a8a5a0488fdb3a7",
   "outputs": [],
   "execution_count": 388
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-21T10:36:42.842231Z",
     "start_time": "2025-02-21T10:36:42.459832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the tokenizer\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.save_pretrained('gpt2_local')"
   ],
   "id": "7ba7cf5411e9620b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_local/tokenizer_config.json',\n",
       " 'gpt2_local/special_tokens_map.json',\n",
       " 'gpt2_local/vocab.json',\n",
       " 'gpt2_local/merges.txt',\n",
       " 'gpt2_local/added_tokens.json')"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 391
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
