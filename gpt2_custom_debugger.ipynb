{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:06:19.792993Z",
     "start_time": "2025-02-23T21:06:19.775509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "\n",
    "print(\"Device: \" + device)"
   ],
   "id": "e39f65fdac2f26be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T21:06:35.662396Z",
     "start_time": "2025-02-23T21:06:33.596071Z"
    }
   },
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, AdamW\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_embd*4)\n",
    "        self.act = nn.GELU()\n",
    "        self.c_proj = nn.Linear(config.n_embd*4, config.n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        return h2\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2Block, self).__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([GPT2Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        x = self.transformer['wte'](x)\n",
    "        pos = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        x = x + self.transformer['wpe'](pos)\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        x = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.reshape(-1))\n",
    "        return x, loss\n",
    "    \n",
    "    \n",
    "model = GPT.from_pretrained('gpt2').to(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:08:39.899864Z",
     "start_time": "2025-02-23T21:08:39.742617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./gpt2_local')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "samples = 5\n",
    "tokens = tokenizer.encode(\"My wife is working as a\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long, device=device) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(samples, 1) # (5, 8)\n",
    "while tokens.shape[1] < 10:\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(tokens)\n",
    "        logits = logits[:, -1, :] # (5, 50257)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk = torch.topk(logits, 5, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 20, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        tokens = torch.cat([tokens, xcol], dim=-1)\n",
    "    \n",
    "for i in range(tokens.shape[0]):    \n",
    "    print(\"> \" + tokenizer.decode(tokens[i].tolist()))"
   ],
   "id": "f23a31afea6df049",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> My wife is working as a software engineer, and\n",
      "> My wife is working as a data scientist.\n",
      "\n",
      "> My wife is working as a software development and AI\n",
      "> My wife is working as a software developer.\n",
      "\n",
      "> My wife is working as a software engineer at a\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:08:37.488070Z",
     "start_time": "2025-02-23T21:08:33.907936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "B, T = 5, 8\n",
    "data = open('women_in_tech.txt').read()\n",
    "data = tokenizer.encode(data)\n",
    "batches = [data[i:i + T + 1] for i in range(0, len(data), T) if len(data[i:i + T + 1]) == T + 1]\n",
    "batches = torch.tensor(batches, dtype=torch.long)\n",
    "if len(batches) % B != 0:\n",
    "    batches = batches[:-(len(batches) % B)]\n",
    "batches = batches.reshape(-1, B, T + 1) # (N, B, T+1)\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    cum_loss = 0\n",
    "    for batch in batches:\n",
    "        x = batch[:, :-1].clone().to(device)\n",
    "        y = batch[:, 1:].clone().to(device)\n",
    "        opt = AdamW(model.parameters(), lr=1e-4)\n",
    "        opt.zero_grad()\n",
    "        logits, loss = model(x, y)\n",
    "        cum_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    cum_loss /= len(batches)\n",
    "    print(f'epoch {epoch} loss: {cum_loss:.2f}')"
   ],
   "id": "b8d76628c66853cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 3.73\n",
      "epoch 1 loss: 2.03\n",
      "epoch 2 loss: 1.16\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T21:13:47.406328Z",
     "start_time": "2025-02-23T21:13:47.032662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input = open(\"women_in_tech.txt\", \"r\").read()\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\").to(device)\n",
    "target_ids = input_ids.clone()[0, 1:]\n",
    "input_ids = input_ids[:, :-1]\n",
    "\n",
    "def get_top_token_ids(rep):\n",
    "    prob = torch.softmax(rep @ model.transformer.wte.weight.T, dim=-1)\n",
    "    indices = torch.argmax(prob, dim=-1)\n",
    "    return indices\n",
    "\n",
    "num_tokens = input_ids.shape[1]\n",
    "num_layers = len(model.transformer.h)\n",
    "rep = torch.zeros((num_layers, num_tokens), device=device, dtype=torch.int64)\n",
    "def hook(_, args, output, idx):\n",
    "    token_idx = output[0].shape[1]\n",
    "    output_vec = output[0][:,:]\n",
    "    rep[idx] = get_top_token_ids(output_vec)\n",
    "\n",
    "hooks = []\n",
    "for i, h in enumerate(model.transformer.h):\n",
    "    hk = h.register_forward_hook(lambda module, args, output, idx=i: hook(module, args, output, idx))\n",
    "    hooks.append(hk)\n",
    "\n",
    "try:\n",
    "    logits, _ = model(input_ids)\n",
    "    logits = logits[0]\n",
    "except Exception as e:\n",
    "    print(\"Error in model call: \",e)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "output_ids = torch.argmax(logits, dim=-1)\n",
    "stablised_layer = torch.argmin((rep - output_ids) ** 2, dim=0).to('cpu')\n",
    "output = [tokenizer.decode(int(i)) for i in output_ids]\n",
    "ip = [tokenizer.decode(int(i)) for i in input_ids[0]]\n",
    "target = [tokenizer.decode(int(i)) for i in target_ids]\n",
    "pd.DataFrame({\"input\": ip,\"output\": output, \"layer\": stablised_layer, \"target\": target})\n",
    "# pd.DataFrame({'layer':stablised_layer}).hist()"
   ],
   "id": "171c4fc45c36c26e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           input       output  layer       target\n",
       "0             My         wife     11         wife\n",
       "1           wife           is      0           is\n",
       "2             is      working      0      working\n",
       "3        working           at      6           at\n",
       "4             at            a      8            a\n",
       "..           ...          ...    ...          ...\n",
       "281     learning            ,      8          and\n",
       "282          and      machine      1      natural\n",
       "283      natural     language     11     language\n",
       "284     language   processing      5   processing\n",
       "285   processing            .      3            .\n",
       "\n",
       "[286 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>layer</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>wife</td>\n",
       "      <td>11</td>\n",
       "      <td>wife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wife</td>\n",
       "      <td>is</td>\n",
       "      <td>0</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>working</td>\n",
       "      <td>0</td>\n",
       "      <td>working</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>working</td>\n",
       "      <td>at</td>\n",
       "      <td>6</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>at</td>\n",
       "      <td>a</td>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>learning</td>\n",
       "      <td>,</td>\n",
       "      <td>8</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>and</td>\n",
       "      <td>machine</td>\n",
       "      <td>1</td>\n",
       "      <td>natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>natural</td>\n",
       "      <td>language</td>\n",
       "      <td>11</td>\n",
       "      <td>language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>language</td>\n",
       "      <td>processing</td>\n",
       "      <td>5</td>\n",
       "      <td>processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>processing</td>\n",
       "      <td>.</td>\n",
       "      <td>3</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
