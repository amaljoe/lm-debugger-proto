{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-10T18:29:42.914738Z",
     "start_time": "2025-02-10T18:29:40.030381Z"
    }
   },
   "source": [
    "from pycparser.ply.yacc import token\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"women_in_tech.jsonl\", split=\"train\")\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"prompt\"] + examples[\"completion\"], truncation=True, padding=\"max_length\", max_length=0)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Fine-tuning arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned-women-tech\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9336518e3e7e4de7af70e23d42f25aaa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named input_ids expected length 20 but got length 40",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtokenize_function\u001B[39m(examples):\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer(examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompletion\u001B[39m\u001B[38;5;124m\"\u001B[39m], truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m---> 18\u001B[0m tokenized_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenize_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Fine-tuning arguments\u001B[39;00m\n\u001B[1;32m     21\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m     22\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./gpt2-finetuned-women-tech\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     23\u001B[0m     per_device_train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m     save_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     28\u001B[0m )\n",
      "File \u001B[0;32m~/Desktop/Workspace/Research/lm-debugger-proto/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:560\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    553\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    558\u001B[0m }\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 560\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    561\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    562\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Workspace/Research/lm-debugger-proto/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3073\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3068\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[1;32m   3069\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3070\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[1;32m   3071\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3072\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3073\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_single\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdataset_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   3074\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   3075\u001B[0m \u001B[43m                \u001B[49m\u001B[43mshards_done\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n",
      "File \u001B[0;32m~/Desktop/Workspace/Research/lm-debugger-proto/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3499\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[1;32m   3497\u001B[0m         writer\u001B[38;5;241m.\u001B[39mwrite_table(batch\u001B[38;5;241m.\u001B[39mto_arrow())\n\u001B[1;32m   3498\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3499\u001B[0m         \u001B[43mwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3500\u001B[0m num_examples_progress_update \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m num_examples_in_batch\n\u001B[1;32m   3501\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m _time \u001B[38;5;241m+\u001B[39m config\u001B[38;5;241m.\u001B[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001B[0;32m~/Desktop/Workspace/Research/lm-debugger-proto/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:608\u001B[0m, in \u001B[0;36mArrowWriter.write_batch\u001B[0;34m(self, batch_examples, writer_batch_size)\u001B[0m\n\u001B[1;32m    606\u001B[0m         inferred_features[col] \u001B[38;5;241m=\u001B[39m typed_sequence\u001B[38;5;241m.\u001B[39mget_inferred_type()\n\u001B[1;32m    607\u001B[0m schema \u001B[38;5;241m=\u001B[39m inferred_features\u001B[38;5;241m.\u001B[39marrow_schema \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpa_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema\n\u001B[0;32m--> 608\u001B[0m pa_table \u001B[38;5;241m=\u001B[39m \u001B[43mpa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001B[0;32m~/Desktop/Workspace/Research/lm-debugger-proto/.venv/lib/python3.12/site-packages/pyarrow/table.pxi:4868\u001B[0m, in \u001B[0;36mpyarrow.lib.Table.from_arrays\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Desktop/Workspace/Research/lm-debugger-proto/.venv/lib/python3.12/site-packages/pyarrow/table.pxi:4214\u001B[0m, in \u001B[0;36mpyarrow.lib.Table.validate\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/Desktop/Workspace/Research/lm-debugger-proto/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mArrowInvalid\u001B[0m: Column 2 named input_ids expected length 20 but got length 40"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:10:32.129183Z",
     "start_time": "2025-02-11T09:10:22.658553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load GPT-2 Medium and Tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a padding token, so we assign one\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Apply LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load dataset (example: Hugging Face \"wikitext\" dataset)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)  # 10% for evaluation\n",
    "tokenized_train_dataset = split[\"train\"]\n",
    "tokenized_eval_dataset = split[\"test\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    evaluation_strategy=\"epoch\",  # Prevents empty evaluation loss\n",
    "    bf16=True  \n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,  # Training dataset\n",
    "    eval_dataset=tokenized_eval_dataset  # ADD this evaluation dataset\n",
    ")\n",
    "\n",
    "\n",
    "# Fine-Tune Model\n",
    "trainer.train()\n",
    "\n",
    "# Save Fine-Tuned Model\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n"
   ],
   "id": "6941bea5d2ccbe1d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 37\u001B[0m\n\u001B[1;32m     34\u001B[0m     inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mcopy()  \u001B[38;5;66;03m# Labels should match input_ids for causal LM\u001B[39;00m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n\u001B[0;32m---> 37\u001B[0m tokenized_train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241m.\u001B[39mmap(preprocess_function, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     38\u001B[0m tokenized_eval_dataset \u001B[38;5;241m=\u001B[39m eval_dataset\u001B[38;5;241m.\u001B[39mmap(preprocess_function, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Training Arguments\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T09:03:49.628967Z",
     "start_time": "2025-02-11T09:03:49.482309Z"
    }
   },
   "cell_type": "code",
   "source": "!which python",
   "id": "6e07333a0587e361",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amaljoe/Desktop/Workspace/Research/lm-debugger-proto/.venv/bin/python\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
