{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T20:55:56.872784Z",
     "start_time": "2025-02-06T20:55:54.323811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ],
   "id": "f416ed2eeb3bdf7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T20:55:57.071746Z",
     "start_time": "2025-02-06T20:55:57.030486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Value Vector Projection\n",
    "\n",
    "def get_top_k_tokens(rep, k=5):\n",
    "    prob = torch.softmax(model.transformer.wte.weight @ rep, dim=-1)\n",
    "    prob, indices = torch.topk(prob, 5)\n",
    "    return [tokenizer.decode(i) for i in indices]\n",
    "\n",
    "def get_value_vector_tokens(layer, dim):\n",
    "    return get_top_k_tokens(t.h[layer].mlp.c_proj.weight[dim])\n",
    "\n",
    "get_value_vector_tokens(layer=0, dim=366)"
   ],
   "id": "901b4c787e84a7be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' behav', 'ngth', 'EStreamFrame', ' disg', 'ften']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 249
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T20:56:09.922286Z",
     "start_time": "2025-02-06T20:56:09.714943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Before and After Layer Representation\n",
    "\n",
    "t: GPT2Model = model.transformer\n",
    "\n",
    "def hook(_, args, output, idx):\n",
    "    print(f\"\\n------Layer {idx}------\")\n",
    "    input_vec = args[0][0,-1,:]\n",
    "    output_vec = output[0][0,-1,:]\n",
    "    print(f\"Input: {get_top_k_tokens(t.ln_f(input_vec))}\")\n",
    "    print(f\"Output: {get_top_k_tokens(t.ln_f(output_vec))}\")\n",
    "\n",
    "hooks = []\n",
    "for i, layer in enumerate(t.h):\n",
    "    h = layer.register_forward_hook(\n",
    "        lambda module, args, output, idx=i: hook(module, args, output, idx)\n",
    "    )\n",
    "    hooks.append(h)\n",
    "    \n",
    "try:\n",
    "    # Run the model to get outputs and capture intermediate representations\n",
    "    input = tokenizer.encode(\"My wife is working as a\", return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input)\n",
    "    logits = outputs.logits\n",
    "    generated_ids = torch.argmax(logits, dim=-1)\n",
    "    generated_text = tokenizer.decode(generated_ids[0][-1])\n",
    "    print(f\"\\nGenerated next token: {generated_text}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Remove the hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ],
   "id": "a557b60a47ac887a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------Layer 0------\n",
      "Input: [' unden', ' helicop', ' streng', ' enthusi', ' notor']\n",
      "Output: [' completely', ' \"', ' fully', ' particularly', ' certain']\n",
      "\n",
      "------Layer 1------\n",
      "Input: [' completely', ' \"', ' fully', ' particularly', ' certain']\n",
      "Output: [' particularly', ' \"', ' single', ' completely', ' fully']\n",
      "\n",
      "------Layer 2------\n",
      "Input: [' particularly', ' \"', ' single', ' completely', ' fully']\n",
      "Output: [' particularly', ' single', ' \"', ' piece', ' very']\n",
      "\n",
      "------Layer 3------\n",
      "Input: [' particularly', ' single', ' \"', ' piece', ' very']\n",
      "Output: [' separate', ' very', ' single', ' well', ' particularly']\n",
      "\n",
      "------Layer 4------\n",
      "Input: [' separate', ' very', ' single', ' well', ' particularly']\n",
      "Output: [' separate', ' member', ' part', ' single', ' well']\n",
      "\n",
      "------Layer 5------\n",
      "Input: [' separate', ' member', ' part', ' single', ' well']\n",
      "Output: [' member', ' part', ' separate', ' host', ' single']\n",
      "\n",
      "------Layer 6------\n",
      "Input: [' member', ' part', ' separate', ' host', ' single']\n",
      "Output: [' part', ' separate', ' member', ' full', ' very']\n",
      "\n",
      "------Layer 7------\n",
      "Input: [' part', ' separate', ' member', ' full', ' very']\n",
      "Output: [' part', ' separate', ' parallel', ' well', ' member']\n",
      "\n",
      "------Layer 8------\n",
      "Input: [' part', ' separate', ' parallel', ' well', ' member']\n",
      "Output: [' part', ' well', ' consultant', ' regular', ' full']\n",
      "\n",
      "------Layer 9------\n",
      "Input: [' part', ' well', ' consultant', ' regular', ' full']\n",
      "Output: [' part', ' well', ' consultant', ' member', ' non']\n",
      "\n",
      "------Layer 10------\n",
      "Input: [' part', ' well', ' consultant', ' member', ' non']\n",
      "Output: [' part', ' well', ' full', ' member', ' non']\n",
      "\n",
      "------Layer 11------\n",
      "Input: [' part', ' well', ' full', ' member', ' non']\n",
      "Output: [' part', ' full', ' non', ' member', ' well']\n",
      "\n",
      "------Layer 12------\n",
      "Input: [' part', ' full', ' non', ' member', ' well']\n",
      "Output: [' consultant', ' freelance', ' part', ' full', ' non']\n",
      "\n",
      "------Layer 13------\n",
      "Input: [' consultant', ' freelance', ' part', ' full', ' non']\n",
      "Output: [' freelance', ' consultant', ' professional', ' graduate', ' non']\n",
      "\n",
      "------Layer 14------\n",
      "Input: [' freelance', ' consultant', ' professional', ' graduate', ' non']\n",
      "Output: [' consultant', ' freelance', ' professional', ' volunteer', ' member']\n",
      "\n",
      "------Layer 15------\n",
      "Input: [' consultant', ' freelance', ' professional', ' volunteer', ' member']\n",
      "Output: [' consultant', ' freelance', ' professional', ' volunteer', ' doctor']\n",
      "\n",
      "------Layer 16------\n",
      "Input: [' consultant', ' freelance', ' professional', ' volunteer', ' doctor']\n",
      "Output: [' freelance', ' consultant', ' professional', ' waitress', ' volunteer']\n",
      "\n",
      "------Layer 17------\n",
      "Input: [' freelance', ' consultant', ' professional', ' waitress', ' volunteer']\n",
      "Output: [' consultant', ' freelance', ' professional', ' waitress', ' volunteer']\n",
      "\n",
      "------Layer 18------\n",
      "Input: [' consultant', ' freelance', ' professional', ' waitress', ' volunteer']\n",
      "Output: [' consultant', ' waitress', ' professional', ' freelance', ' teacher']\n",
      "\n",
      "------Layer 19------\n",
      "Input: [' consultant', ' waitress', ' professional', ' freelance', ' teacher']\n",
      "Output: [' waitress', ' nurse', ' consultant', ' freelance', ' teacher']\n",
      "\n",
      "------Layer 20------\n",
      "Input: [' waitress', ' nurse', ' consultant', ' freelance', ' teacher']\n",
      "Output: [' nurse', ' waitress', ' teacher', ' consultant', ' freelance']\n",
      "\n",
      "------Layer 21------\n",
      "Input: [' nurse', ' waitress', ' teacher', ' consultant', ' freelance']\n",
      "Output: [' nurse', ' waitress', ' teacher', ' consultant', ' lawyer']\n",
      "\n",
      "------Layer 22------\n",
      "Input: [' nurse', ' waitress', ' teacher', ' consultant', ' lawyer']\n",
      "Output: [' nurse', ' waitress', ' teacher', ' professional', ' lawyer']\n",
      "\n",
      "------Layer 23------\n",
      "Input: [' nurse', ' waitress', ' teacher', ' professional', ' lawyer']\n",
      "Output: [' nurse', ' teacher', ' waitress', ' lawyer', ' consultant']\n",
      "\n",
      "Generated next token:  nurse\n"
     ]
    }
   ],
   "execution_count": 251
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
