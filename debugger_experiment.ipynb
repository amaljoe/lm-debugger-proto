{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T13:27:06.484416Z",
     "start_time": "2025-02-22T13:27:05.180166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "t: GPT2Model = model.transformer"
   ],
   "id": "f416ed2eeb3bdf7d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T08:32:40.066781Z",
     "start_time": "2025-02-20T08:32:40.026003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Value Vector Projection\n",
    "\n",
    "def get_top_k_tokens(rep, k=5):\n",
    "    prob = torch.softmax(model.transformer.wte.weight @ rep, dim=-1)\n",
    "    prob, indices = torch.topk(prob, 5)\n",
    "    return [tokenizer.decode(i) for i in indices]\n",
    "\n",
    "def get_value_vector_tokens(layer, dim):\n",
    "    return get_top_k_tokens(t.h[layer].mlp.c_proj.weight[dim])\n",
    "\n",
    "get_value_vector_tokens(layer=17, dim=2940)"
   ],
   "id": "901b4c787e84a7be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cold', ' colder', ' precipitation', ' frost', 'clone']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T13:34:51.709347Z",
     "start_time": "2025-02-22T13:34:51.707067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_k_token_indices(rep, k=5):\n",
    "    prob = torch.softmax(model.transformer.wte.weight @ rep, dim=-1)\n",
    "    prob, indices = torch.topk(prob, 5)\n",
    "    return indices"
   ],
   "id": "5dbc92684fdffff6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T13:49:36.959262Z",
     "start_time": "2025-02-22T13:49:34.649225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Before and After Layer Representation\n",
    "import numpy as np\n",
    "\n",
    "max_tokens = 20\n",
    "rep = np.zeros((max_tokens, len(t.h)))\n",
    "\n",
    "\n",
    "def hook(_, args, output, idx):\n",
    "    token_idx = output[0].shape[1]\n",
    "    input_vec = args[0][0,-1,:]\n",
    "    output_vec = output[0][0,-1,:]\n",
    "    rep[token_idx][idx] = get_top_k_token_indices(t.ln_f(output_vec))[0]\n",
    "          \n",
    "\n",
    "hooks = []\n",
    "for i, layer in enumerate(t.h[:]):\n",
    "    h1 = layer.register_forward_hook(\n",
    "        lambda module, args, output, idx=i: hook(module, args, output, idx)\n",
    "    )\n",
    "    \n",
    "    hooks.append(h1)\n",
    "\n",
    "try:\n",
    "    input = tokenizer.encode(\"My wife is working as a\", return_tensors=\"pt\")\n",
    "    output_ids = np.zeros(max_tokens)\n",
    "    for i in range(input.shape[1]):\n",
    "        output_ids[i] = input[0][i]\n",
    "    for i in range(input.shape[1], max_tokens):\n",
    "        # Run the model to get outputs and capture intermediate representations\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input)\n",
    "        logits = outputs.logits\n",
    "        generated_ids = torch.argmax(logits, dim=-1)\n",
    "        generated_text = tokenizer.decode(generated_ids[0][-1])\n",
    "        input = torch.cat([input, generated_ids[0][-1].unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        output_ids[i] = generated_ids[0][-1]\n",
    "    \n",
    "    print(tokenizer.decode(input[0]))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Remove the hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "i = 7\n",
    "finished = np.argmin((rep - np.reshape(output_ids, (max_tokens, 1))) ** 2, axis=1)\n",
    "text = [tokenizer.decode(int(i)) for i in output_ids]\n",
    "pd.DataFrame({\"token\": text, \"layer\": finished})"
   ],
   "id": "a557b60a47ac887a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife is working as a nurse and I'm a teacher. We have a small house and we\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
